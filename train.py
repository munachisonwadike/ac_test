from glob import glob
from librosa.feature import melspectrogram
from PIL import Image
from skimage.transform import resize
from tensorflow.keras.optimizers import Adam
from tensorflow.python.framework import tensor_shape
from torchaudio.transforms import MelScale, Spectrogram
from tqdm import tqdm

from losses import *
from model import *

import datetime
import heapq
import IPython
import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np
import os
import random
import soundfile as sf
import tensorflow.keras.backend as K
import tensorflow as tf
import time
import torch
import torch.nn as nn
import torch.nn.functional as F

print(tf.__version__) # original melganvc notebook used 2.1
# sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))
# exit()
#Hyperparameters

hop=192               #hop size (window size = 6*hop)
sr=16000              #sampling rate
min_level_db=-100     #reference values to normalize data
ref_level_db=20

shape=24              #length of time axis of split specrograms to feed to generator            
vec_len=128           #length of vector generated by siamese vector
bs = 16               #batch size
delta = 2.            #constant for siamese loss

#There seems to be a problem with Tensorflow STFT, so we'll be using pytorch to handle offline mel-spectrogram generation and waveform reconstruction
#For waveform reconstruction, a gradient-based method is used:

''' Decorsière, Rémi, Peter L. Søndergaard, Ewen N. MacDonald, and Torsten Dau. 
"Inversion of auditory spectrograms, traditional spectrograms, and other envelope representations." 
IEEE/ACM Transactions on Audio, Speech, and Language Processing 23, no. 1 (2014): 46-56.'''

#ORIGINAL CODE FROM https://github.com/yoyololicon/spectrogram-inversion

# waveform array from path of folder containing wav files
def audio_array(path):
	ls = glob(f'{path}/*.wav')
	adata = []
	for i in range(len(ls)):
		x, sr = tf.audio.decode_wav(tf.io.read_file(ls[i]), 1)
		x = np.array(x, dtype=np.float32)
		adata.append(x)
	return np.array(adata)



torch.set_default_tensor_type('torch.cuda.FloatTensor')

specobj = Spectrogram(n_fft=6*hop, win_length=6*hop, hop_length=hop, pad=0, power=2, normalized=True)
specfunc = specobj.forward
melobj = MelScale(n_mels=hop, sample_rate=sr, f_min=0.)
melfunc = melobj.forward

def melspecfunc(waveform):
	specgram = specfunc(waveform)
	mel_specgram = melfunc(specgram)
	return mel_specgram

def spectral_convergence(input, target):

	return 20 * ((input - target).norm().log10() - target.norm().log10())

def GRAD(spec, 
	transform_fn, 
	samples=None, 
	init_x0=None, 
	maxiter=1000, 
	tol=1e-6, 
	verbose=1,
	evaiter=10, 
	lr=0.003):

	spec = torch.Tensor(spec)
	samples = (spec.shape[-1]*hop)-hop

	if init_x0 is None:
		init_x0 = spec.new_empty((1,samples)).normal_(std=1e-6)
	x = nn.Parameter(init_x0)
	T = spec

	criterion = nn.L1Loss()
	optimizer = torch.optim.Adam([x], lr=lr)

	bar_dict = {}
	metric_func = spectral_convergence
	bar_dict['spectral_convergence'] = 0
	metric = 'spectral_convergence'

	init_loss = None
	with tqdm(total=maxiter, disable=not verbose) as pbar:
		for i in range(maxiter):
			optimizer.zero_grad()
			V = transform_fn(x)
			loss = criterion(V, T)
			loss.backward()
			optimizer.step()
			lr = lr*0.9999
			for param_group in optimizer.param_groups:
				param_group['lr'] = lr

			if i % evaiter == evaiter - 1:
				with torch.no_grad():
					V = transform_fn(x)
					bar_dict[metric] = metric_func(V, spec).item()
					l2_loss = criterion(V, spec).item()
					pbar.set_postfix(**bar_dict, loss=l2_loss)
					pbar.update(evaiter)

	return x.detach().view(-1).cpu()

def normalize(S):


	return np.clip((((S - min_level_db) / -min_level_db)*2.)-1., -1, 1)

def denormalize(S):

	return (((np.clip(S, -1, 1)+1.)/2.) * -min_level_db) + min_level_db

def prep(wv, hop=192):
	S = np.array(torch.squeeze(melspecfunc(torch.Tensor(wv).view(1,-1))).detach().cpu())
	S = librosa.power_to_db(S)-ref_level_db
	return normalize(S)

def deprep(S):
	S = denormalize(S)+ref_level_db
	S = librosa.db_to_power(S)
	wv = GRAD(np.expand_dims(S,0), melspecfunc, maxiter=2000, evaiter=10, tol=1e-8)
	return np.array(np.squeeze(wv))


# helper functions
# generate spectrograms from waveform array
def tospec(data):
	specs = np.empty(data.shape[0], dtype=object)
	for i in range(data.shape[0]):
		x = data[i]
		S = prep(x)
		S = np.array(S, dtype=np.float32)
		specs[i] = np.expand_dims(S, -1)
	print(specs.shape)
	return specs




# concatenate spectrograms in array along the time axis
def testass(a):
	but=False
	con = np.array([])
	nim = a.shape[0]
	for i in range(nim):
		im = a[i]
		im = np.squeeze(im)
		if not but:
			con=im
			but=True
		else:
			con = np.concatenate((con,im), axis=1)
	return np.squeeze(con)

# split spectrograms in chunks with equal size
def splitcut(data):
	ls = []
	mini = 0
	minifinal = 10*shape                                                              #max spectrogram length
	for i in range(data.shape[0]-1):
		if data[i].shape[1]<=data[i+1].shape[1]:
			mini = data[i].shape[1]
		else:
			mini = data[i+1].shape[1]
		if mini>=3*shape and mini<minifinal:
			minifinal = mini
	for i in range(data.shape[0]):
		x = data[i]
		if x.shape[1]>=3*shape:
			for n in range(x.shape[1]//minifinal):
				ls.append(x[:,n*minifinal:n*minifinal+minifinal,:])
			ls.append(x[:,-minifinal:,:])
	return np.array(ls)

# generating Mel-Spectrogram dataset (uncomment where needed)
#adata: source spectrograms 	
#bdata: target spectrograms

#MALE1
awv = audio_array('/home/datasets/festvox_datasets/cmu_us_clb_arctic/wav')                               #get waveform array from folder containing wav files
aspec = tospec(awv)                                                                 #get spectrogram array
adata = splitcut(aspec)                                                             #split spectrogams to fixed length
print(K.shape(adata))
#FEMALE1
bwv = audio_array('/home/datasets/festvox_datasets/cmu_us_bdl_arctic/wav')
bspec = tospec(bwv)
bdata = splitcut(bspec)
print(K.shape(bdata))

# #MALE2
# awv = audio_array('../content/cmu_us_rms_arctic/wav')
# aspec = tospec(awv)
# adata = splitcut(aspec)
# #FEMALE2
# bwv = audio_array('../content/cmu_us_slt_arctic/wav')
# bspec = tospec(bwv)
# bdata = splitcut(bspec)

#JAZZ MUSIC
# awv = audio_array('../content/genres/jazz')
# aspec = tospec(awv)
# adata = splitcut(aspec)
#CLASSICAL MUSIC
# bwv = audio_array('../content/genres/classical')
# bspec = tospec(bwv)
# bdata = splitcut(bspec)



# creating Tensorflow datasets
@tf.function
def proc(x):
  return tf.image.random_crop(x, size=[hop, 3*shape, 1])
dsa = tf.data.Dataset.from_tensor_slices(adata).repeat(50).map(proc, num_parallel_calls=tf.data.experimental.AUTOTUNE).shuffle(10000).batch(bs, drop_remainder=True)
dsb = tf.data.Dataset.from_tensor_slices(bdata).repeat(50).map(proc, num_parallel_calls=tf.data.experimental.AUTOTUNE).shuffle(10000).batch(bs, drop_remainder=True)




# generate a random batch to display current training results
def testgena():
	sw = True
	while sw:
		a = np.random.choice(aspec)
		if a.shape[1]//shape != 1:
			sw=False
	dsa = []
	if a.shape[1]//shape > 6:
		num = 6
	else:
		num = a.shape[1]//shape
	rn = np.random.randint(a.shape[1]-(num*shape))
	for i in range(num):
		im = a[:,rn+(i*shape):rn+(i*shape)+shape]
		im = np.reshape(im, (im.shape[0],im.shape[1],1))
		dsa.append(im)
	return np.array(dsa, dtype=np.float32)

# show results mid-training
def save_test_image_full(path):
	a = testgena()
	print(a.shape)
	ab = gen(a, training=False)
	ab = testass(ab)
	a = testass(a)
	abwv = deprep(ab)
	awv = deprep(a)
	sf.write(path+'/soundfile1.wav', abwv, sr)
	sf.write(path+'/soundfile2.wav', abwv, sr)
	# IPython.display.display(IPython.display.Audio(np.squeeze(abwv), rate=sr))
	# IPython.display.display(IPython.display.Audio(np.squeeze(awv), rate=sr))
	fig, axs = plt.subplots(ncols=2)
	axs[0].imshow(np.flip(a, -2), cmap=None)
	axs[0].axis('off')
	axs[0].set_title('Source')
	axs[1].imshow(np.flip(ab, -2), cmap=None)
	axs[1].axis('off')
	axs[1].set_title('Generated')
	# plt.show()
	plt.tight_layout()
	plt.savefig(path+'/rand_testimg.jpg')


# save in training loop
def save_end(epoch, gloss, closs, mloss, n_save=3, save_path='/home/munachisnwadike_gmail_com/melganvc/saves'):                 #use custom save_path (i.e. Drive '../content/drive/My Drive/')
	if epoch % n_save == 0:
		print('Saving...')
		# path = f'{save_path}/g{str(gloss)[:5]}-c{str(closs)[:5]}-m{str(mloss)[:5]}'
		path = f'{save_path}/epoch-{str(epoch)}'
		os.mkdir(path)
		gen.save_weights(path+'/gen.h5')
		critic.save_weights(path+'/critic.h5')
		siam.save_weights(path+'/siam.h5')
		save_test_image_full(path)



# get models and optimizers
def get_networks(shape, load_model=False, path=None):
	if not load_model:
		gen, critic, siam = build(vec_len, hop, shape)
	else:
		gen, critic, siam = load(path, vec_len, hop, shape)
	print('Built networks')
	opt_gen = Adam(0.0001, 0.5)
	opt_disc = Adam(0.0001, 0.5)
	
	return gen, critic, siam, [opt_gen, opt_disc]




# training functions
# train generator, siamese and critic
@tf.function
def train_all(a, b, delta):
	#splitting spectrogram in 3 parts
	aa, aa2, aa3 = extract_image(a) 
	bb, bb2, bb3 = extract_image(b)

	with tf.GradientTape() as tape_gen, tf.GradientTape() as tape_disc:

		# translating A to B
		fab = gen(aa, training=True)
		fab2 = gen(aa2, training=True)
		fab3 = gen(aa3, training=True)
		
		# identity mapping B to B                                                        COMMENT THESE 3 LINES IF THE IDENTITY LOSS TERM IS NOT NEEDED
		fid = gen(bb, training=True) 
		fid2 = gen(bb2, training=True)
		fid3 = gen(bb3, training=True)
		
		# concatenate/assemble converted spectrograms
		fabtot = assemble_image([fab, fab2, fab3])

		# feed concatenated spectrograms to critic
		cab = critic(fabtot, training=True)
		cb = critic(b, training=True)

		# feed 2 pairs (A,G(A)) extracted spectrograms to Siamese
		sab = siam(fab, training=True)
		sab2 = siam(fab3, training=True)
		sa = siam(aa, training=True)
		sa2 = siam(aa3, training=True)

		# identity mapping loss
		loss_id = (mae(bb, fid)+mae(bb2, fid2)+mae(bb3, fid3))/3.                         #loss_id = 0. IF THE IDENTITY LOSS TERM IS NOT NEEDED
		
		# travel loss
		loss_m = loss_travel(sa, sab, sa2, sab2)+loss_siamese(sa, sa2, delta)
		
		# generator and critic losses
		loss_g = g_loss_f(cab)
		loss_dr = d_loss_r(cb)
		loss_df = d_loss_f(cab)
		loss_d = (loss_dr+loss_df)/2.
		
		# generator+siamese total loss
		lossgtot = loss_g+10.*loss_m+0.5*loss_id                                       #CHANGE LOSS WEIGHTS HERE  (COMMENT OUT +w*loss_id IF THE IDENTITY LOSS TERM IS NOT NEEDED)
  
	#computing and applying gradients
	grad_gen = tape_gen.gradient(lossgtot, gen.trainable_variables+siam.trainable_variables)
	opt_gen.apply_gradients(zip(grad_gen, gen.trainable_variables+siam.trainable_variables))

	grad_disc = tape_disc.gradient(loss_d, critic.trainable_variables)
	opt_disc.apply_gradients(zip(grad_disc, critic.trainable_variables))
  
	return loss_dr, loss_df, loss_g, loss_id

# train critic only
@tf.function
def train_d(a,b):
	aa, aa2, aa3 = extract_image(a)
	with tf.GradientTape() as tape_disc:

		fab = gen(aa, training=True)
		fab2 = gen(aa2, training=True)
		fab3 = gen(aa3, training=True)
		fabtot = assemble_image([fab,fab2,fab3])

		cab = critic(fabtot, training=True)
		cb = critic(b, training=True)

		loss_dr = d_loss_r(cb)
		loss_df = d_loss_f(cab)

		loss_d = (loss_dr+loss_df)/2.
  
	grad_disc = tape_disc.gradient(loss_d, critic.trainable_variables)
	opt_disc.apply_gradients(zip(grad_disc, critic.trainable_variables))

	return loss_dr, loss_df






# training Loop
def train(epochs, delta, batch_size=16, lr=0.0001, n_save=6, gupt=5):

	opt_gen.learning_rate = lr
	opt_disc.learning_rate = lr


	df_list = []
	dr_list = []
	g_list = []
	id_list = []
	c = 0
	g = 0
	
	for epoch in range(epochs):
		bef = time.time()

		for batchi, (a, b) in enumerate(zip(dsa, dsb)):

			if batchi%gupt==0:
				dloss_t, dloss_f, gloss, idloss = train_all(a, b, delta)
			else:
				dloss_t, dloss_f = train_d(a,b)

			df_list.append(dloss_f)
			dr_list.append(dloss_t)
			g_list.append(gloss)
			id_list.append(idloss)
			c += 1
			g += 1


			if batchi%600==0:
				print(f'[Epoch {epoch}/{epochs}] [Batch {batchi}] [D loss f: {np.mean(df_list[-g:], axis=0)} ', end='')
				print(f'r: {np.mean(dr_list[-g:], axis=0)}] ', end='')
				print(f'[G loss: {np.mean(g_list[-g:], axis=0)}] ', end='')
				print(f'[ID loss: {np.mean(id_list[-g:])}] ', end='')
				print(f'[LR: {lr}]')
				g = 0
			nbatch=batchi

		print(f'Time/Batch {(time.time()-bef)/nbatch}')
		save_end(epoch,  np.mean(g_list[-n_save*c:], axis=0), np.mean(df_list[-n_save*c:], axis=0), np.mean(id_list[-n_save*c:], axis=0), n_save=n_save)
		print(f'Mean D loss: {np.mean(df_list[-c:], axis=0)} Mean G loss: {np.mean(g_list[-c:], axis=0)} Mean ID loss: {np.mean(id_list[-c:], axis=0)}')
		c = 0
                      


#Build models and initialize optimizers

#If load_model=True, specify the path where the models are saved
gen, critic, siam, [opt_gen,opt_disc] = get_networks(shape, load_model=False, path='../content/drive/My Drive/')



#Training
#n_save = how many epochs between each saving and displaying of results
#gupt = how many discriminator updates for generator+siamese update
train(5000, delta, batch_size=bs, lr=0.0002, n_save=1, gupt=3)