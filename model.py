from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Concatenate, Conv2D, Conv2DTranspose, GlobalAveragePooling2D, UpSampling2D, LeakyReLU, ReLU, Add, Multiply, Lambda, Dot, BatchNormalization, Activation, ZeroPadding2D, Cropping2D, Cropping1D
from tensorflow.keras.models import Sequential, Model, load_model
from tensorflow.python.eager import context
from tensorflow.python.keras.utils import conv_utils
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import gen_math_ops
from tensorflow.python.ops import math_ops
from tensorflow.python.ops import sparse_ops
from tensorflow.python.ops import standard_ops

import tensorflow as tf
import tensorflow.keras.backend as K

# adding Spectral Normalization to convolutional layers
def l2normalize(v, eps=1e-12):
	return v / (tf.norm(v) + eps)

class ConvSN2D(tf.keras.layers.Conv2D):
	def __init__(self, filters, kernel_size, power_iterations=1, **kwargs):
		super(ConvSN2D, self).__init__(filters, kernel_size, **kwargs)
		self.power_iterations = power_iterations

	def build(self, input_shape):
		super(ConvSN2D, self).build(input_shape)

		if self.data_format == 'channels_first':
			channel_axis = 1
		else:
			channel_axis = -1

		self.u = self.add_weight(self.name + '_u',
           	shape=tuple([1, self.kernel.shape.as_list()[-1]]), 
            initializer=tf.initializers.RandomNormal(0, 1),
            trainable=False
        )

	def compute_spectral_norm(self, W, new_u, W_shape):
		for _ in range(self.power_iterations):
			new_v = l2normalize(tf.matmul(new_u, tf.transpose(W)))
			new_u = l2normalize(tf.matmul(new_v, W))
            
		sigma = tf.matmul(tf.matmul(new_v, W), tf.transpose(new_u))
		W_bar = W/sigma

		with tf.control_dependencies([self.u.assign(new_u)]):
			W_bar = tf.reshape(W_bar, W_shape)

		return W_bar


	def call(self, inputs):
		W_shape = self.kernel.shape.as_list()
		W_reshaped = tf.reshape(self.kernel, (-1, W_shape[-1]))
		new_kernel = self.compute_spectral_norm(W_reshaped, self.u, W_shape)
		outputs = self._convolution_op(inputs, new_kernel)

		if self.use_bias:
			if self.data_format == 'channels_first':
				outputs = tf.nn.bias_add(outputs, self.bias, data_format='NCHW')
			else:
				outputs = tf.nn.bias_add(outputs, self.bias, data_format='NHWC')
		if self.activation is not None:
			return self.activation(outputs)

		return outputs

class ConvSN2DTranspose(tf.keras.layers.Conv2DTranspose):
	
	def __init__(self, filters, kernel_size, power_iterations=1, **kwargs):
		super(ConvSN2DTranspose, self).__init__(filters, kernel_size, **kwargs)
		self.power_iterations = power_iterations


	def build(self, input_shape):
		super(ConvSN2DTranspose, self).build(input_shape)

		if self.data_format == 'channels_first':
			channel_axis = 1
		else:
			channel_axis = -1

		self.u = self.add_weight(self.name + '_u',
            shape=tuple([1, self.kernel.shape.as_list()[-1]]), 
            initializer=tf.initializers.RandomNormal(0, 1),
            trainable=False
        )

	def compute_spectral_norm(self, W, new_u, W_shape):
		for _ in range(self.power_iterations):
			new_v = l2normalize(tf.matmul(new_u, tf.transpose(W)))
			new_u = l2normalize(tf.matmul(new_v, W))
            
		sigma = tf.matmul(tf.matmul(new_v, W), tf.transpose(new_u))
		W_bar = W/sigma

		with tf.control_dependencies([self.u.assign(new_u)]):
			W_bar = tf.reshape(W_bar, W_shape)
		
		return W_bar

	def call(self, inputs):
		W_shape = self.kernel.shape.as_list()
		W_reshaped = tf.reshape(self.kernel, (-1, W_shape[-1]))
		new_kernel = self.compute_spectral_norm(W_reshaped, self.u, W_shape)
		inputs_shape = array_ops.shape(inputs)
		batch_size = inputs_shape[0]
		if self.data_format == 'channels_first':
			h_axis, w_axis = 2, 3
		else:
			h_axis, w_axis = 1, 2
		height, width = inputs_shape[h_axis], inputs_shape[w_axis]
		kernel_h, kernel_w = self.kernel_size
		stride_h, stride_w = self.strides

		if self.output_padding is None:
			out_pad_h = out_pad_w = None
		else:
			out_pad_h, out_pad_w = self.output_padding

		out_height = conv_utils.deconv_output_length(height,
                                                    kernel_h,
                                                    padding=self.padding,
                                                    output_padding=out_pad_h,
                                                    stride=stride_h,
                                                    dilation=self.dilation_rate[0])
		out_width = conv_utils.deconv_output_length(width,
                                                    kernel_w,
                                                    padding=self.padding,
                                                    output_padding=out_pad_w,
                                                    stride=stride_w,
                                                    dilation=self.dilation_rate[1])
		if self.data_format == 'channels_first':
			output_shape = (batch_size, self.filters, out_height, out_width)
		else:
			output_shape = (batch_size, out_height, out_width, self.filters)

		output_shape_tensor = array_ops.stack(output_shape)
		outputs = K.conv2d_transpose(
            inputs,
            new_kernel,
            output_shape_tensor,
            strides=self.strides,
            padding=self.padding,
            data_format=self.data_format,
            dilation_rate=self.dilation_rate)

		if not context.executing_eagerly():
			out_shape = self.compute_output_shape(inputs.shape)
			outputs.set_shape(out_shape)

		if self.use_bias:
			outputs = tf.nn.bias_add(
              outputs,
              self.bias,
              data_format=conv_utils.convert_data_format(self.data_format, ndim=4))

		if self.activation is not None:
			return self.activation(outputs)
		return outputs  

class DenseSN(Dense):
	def build(self, input_shape):
		super(DenseSN, self).build(input_shape)
		
		self.u = self.add_weight(self.name + '_u',
            shape=tuple([1, self.kernel.shape.as_list()[-1]]), 
            initializer=tf.initializers.RandomNormal(0, 1),
            trainable=False)
        
	def compute_spectral_norm(self, W, new_u, W_shape):
		new_v = l2normalize(tf.matmul(new_u, tf.transpose(W)))
		new_u = l2normalize(tf.matmul(new_v, W))
		sigma = tf.matmul(tf.matmul(new_v, W), tf.transpose(new_u))
		W_bar = W/sigma
		with tf.control_dependencies([self.u.assign(new_u)]):
			W_bar = tf.reshape(W_bar, W_shape)
		return W_bar
        
	def call(self, inputs):
		W_shape = self.kernel.shape.as_list()
		W_reshaped = tf.reshape(self.kernel, (-1, W_shape[-1]))
		new_kernel = self.compute_spectral_norm(W_reshaped, self.u, W_shape)
		rank = len(inputs.shape)
		if rank > 2:
			outputs = standard_ops.tensordot(inputs, new_kernel, [[rank - 1], [0]])
			if not context.executing_eagerly():
				shape = inputs.shape.as_list()
				output_shape = shape[:-1] + [self.units]
				outputs.set_shape(output_shape)
		else:
			inputs = math_ops.cast(inputs, self._compute_dtype)
			if K.is_sparse(inputs):
				outputs = sparse_ops.sparse_tensor_dense_matmul(inputs, new_kernel)
			else:
				outputs = gen_math_ops.mat_mul(inputs, new_kernel)
		if self.use_bias:
			outputs = tf.nn.bias_add(outputs, self.bias)
		if self.activation is not None:
			return self.activation(outputs)
		return outputs


#Networks Architecture
init = tf.keras.initializers.he_uniform()

def conv2d(layer_input, filters, kernel_size=4, strides=2, padding='same', leaky=True, bnorm=True, sn=True):
	if leaky:
		Activ = LeakyReLU(alpha=0.2)
	else:
		Activ = ReLU()
	if sn:
		d = ConvSN2D(filters, kernel_size=kernel_size, strides=strides, padding=padding, kernel_initializer=init, use_bias=False)(layer_input)
	else:
		d = Conv2D(filters, kernel_size=kernel_size, strides=strides, padding=padding, kernel_initializer=init, use_bias=False)(layer_input)
	if bnorm:
		d = BatchNormalization()(d)
	d = Activ(d)
	return d

def deconv2d(layer_input, layer_res, filters, kernel_size=4, conc=True, scalev=False, bnorm=True, up=True, padding='same', strides=2):
	if up:
		u = UpSampling2D((1,2))(layer_input)
		u = ConvSN2D(filters, kernel_size, strides=(1,1), kernel_initializer=init, use_bias=False, padding=padding)(u)
	else:
		u = ConvSN2DTranspose(filters, kernel_size, strides=strides, kernel_initializer=init, use_bias=False, padding=padding)(layer_input)
	if bnorm:
		u = BatchNormalization()(u)
	u = LeakyReLU(alpha=0.2)(u)
	if conc:
		u = Concatenate()([u,layer_res])
	return u

#Extract function: splitting spectrograms
def extract_image(im):
	im1 = Cropping2D(((0,0), (0, 2*(im.shape[2]//3))))(im)
	im2 = Cropping2D(((0,0), (im.shape[2]//3,im.shape[2]//3)))(im)
	im3 = Cropping2D(((0,0), (2*(im.shape[2]//3), 0)))(im)
	return im1,im2,im3

#Assemble function: concatenating spectrograms
def assemble_image(lsim):
	im1,im2,im3 = lsim
	imh = Concatenate(2)([im1,im2,im3])
	return imh

#U-NET style architecture
def build_generator(input_shape):
	h,w,c = input_shape
	inp = Input(shape=input_shape)
	#downscaling
	g0 = tf.keras.layers.ZeroPadding2D((0,1))(inp)
	g1 = conv2d(g0, 256, kernel_size=(h,3), strides=1, padding='valid')
	g2 = conv2d(g1, 256, kernel_size=(1,9), strides=(1,2))
	g3 = conv2d(g2, 256, kernel_size=(1,7), strides=(1,2))
	#upscaling
	g4 = deconv2d(g3,g2, 256, kernel_size=(1,7), strides=(1,2))
	g5 = deconv2d(g4,g1, 256, kernel_size=(1,9), strides=(1,2), bnorm=False)
	g6 = ConvSN2DTranspose(1, kernel_size=(h,1), strides=(1,1), kernel_initializer=init, padding='valid', activation='tanh')(g5)
	return Model(inp,g6, name='G')

#Siamese Network
def build_siamese(input_shape, vec_len):
	h,w,c = input_shape
	inp = Input(shape=input_shape)
	g1 = conv2d(inp, 256, kernel_size=(h,3), strides=1, padding='valid', sn=False)
	g2 = conv2d(g1, 256, kernel_size=(1,9), strides=(1,2), sn=False)
	g3 = conv2d(g2, 256, kernel_size=(1,7), strides=(1,2), sn=False)
	g4 = Flatten()(g3)
	g5 = Dense(vec_len)(g4)
	return Model(inp, g5, name='S')

#Discriminator (Critic) Network
def build_critic(input_shape):
	h,w,c = input_shape
	inp = Input(shape=input_shape)
	g1 = conv2d(inp, 512, kernel_size=(h,3), strides=1, padding='valid', bnorm=False)
	g2 = conv2d(g1, 512, kernel_size=(1,9), strides=(1,2), bnorm=False)
	g3 = conv2d(g2, 512, kernel_size=(1,7), strides=(1,2), bnorm=False)
	g4 = Flatten()(g3)
	g4 = DenseSN(1, kernel_initializer=init)(g4)
	return Model(inp, g4, name='C')




#Load past models from path to resume training or test
def load(path, vec_len, hop, shape):
	gen = build_generator((hop,shape,1))
	siam = build_siamese((hop,shape,1), vec_len)
	critic = build_critic((hop,3*shape,1))
	gen.load_weights(path+'/gen.h5')
	critic.load_weights(path+'/critic.h5')
	siam.load_weights(path+'/siam.h5')
	return gen, critic, siam

#Build models
def build(vec_len, hop, shape):
	gen = build_generator((hop,shape,1))
	siam = build_siamese((hop,shape,1), vec_len)
	critic = build_critic((hop,3*shape,1))                                          #the discriminator accepts as input spectrograms of triple the width of those generated by the generator
	return gen, critic, siam
